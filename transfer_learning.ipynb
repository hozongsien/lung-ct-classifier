{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:23:10.839939Z",
     "iopub.status.busy": "2020-09-17T01:23:10.839289Z",
     "iopub.status.idle": "2020-09-17T01:23:18.333141Z",
     "shell.execute_reply": "2020-09-17T01:23:18.332497Z"
    },
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk"
   ]
  },
  {
   "source": [
    "# GPU Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable first GPU\n",
    "  tf.config.set_visible_devices(physical_devices[:-1], 'GPU')\n",
    "  logical_devices = tf.config.list_logical_devices('GPU')\n",
    "  # Logical device was not created for first GPU\n",
    "  assert len(logical_devices) == len(physical_devices) - 1\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\nSome of your GPUs may run slowly with dtype policy mixed_float16 because they do not all have compute capability of at least 7.0. Your GPUs:\n  Tesla V100S-PCIE-32GB, compute capability 7.0\n  Tesla V100-PCIE-32GB, compute capability 7.0\n  Tesla P100-PCIE-16GB, compute capability 6.0 (x2)\nSee https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\nIf you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v77rlkCKW0IJ"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "# python -m tensorflow_datasets.scripts.download_and_prepare --datasets=mri_dataset --module_import=mri_dataset --manual_dir=../../data/mri_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds, batch_size, ds_type='train'):\n",
    "    num_classes = 3\n",
    "\n",
    "    resize_and_rescale = create_resize_and_rescale_layer()\n",
    "    data_augmentation = create_augmentation_layer()\n",
    "\n",
    "    if ds_type == 'test':\n",
    "        ds = ds.map(lambda ds: (ds['image'], ds['label']))\n",
    "\n",
    "    ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if ds_type == 'train':\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if ds_type == 'train' or ds_type == 'valid': \n",
    "        ds = ds.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)))\n",
    "    \n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_resize_and_rescale_layer(): \n",
    "    resize_and_rescale = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE[0], IMG_SIZE[1], interpolation='bilinear'),\n",
    "        tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
    "    ], name='resize_and_rescale')\n",
    "    return resize_and_rescale\n",
    "\n",
    "\n",
    "def create_augmentation_layer():\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    ], name='data_augmentation')\n",
    "    # TODO: sheer\n",
    "    # TODO: Gaussian Filter\n",
    "    return data_augmentation\n",
    "\n",
    "\n",
    "class RandomSheer(tf.keras.layers.Layer):\n",
    "  def __init__(self, intensity=0.1, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.intensity = intensity\n",
    "\n",
    "  def call(self, x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class RandomGaussionFilter(tf.keras.layers.Layer):\n",
    "  def __init__(self, mean=0, std=1.0, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  def call(self, x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_base_model(img_shape):\n",
    "    # base_model = tf.keras.applications.MobileNetV2(\n",
    "    #     input_shape=img_shape,\n",
    "    #     include_top=False,\n",
    "    #     weights='imagenet'\n",
    "    # )\n",
    "    # base_model = tf.keras.applications.EfficientNetB7(\n",
    "    #     input_shape=img_shape,\n",
    "    #     include_top=False,\n",
    "    #     weights='imagenet',\n",
    "    # )\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        input_shape=img_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "    ) \n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "\n",
    "def create_prediction_layer(num_classes):\n",
    "    # Float 32\n",
    "    # prediction = tf.keras.Sequential([\n",
    "    #     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    #     tf.keras.layers.Dropout(0.5),\n",
    "    #     tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
    "    # ], name='prediction')\n",
    "\n",
    "    # Mixed precision \n",
    "    prediction = tf.keras.Sequential([\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(num_classes, name='dense_logits'),\n",
    "        tf.keras.layers.Activation('softmax', dtype='float32', name='probabilities'),\n",
    "    ], name='prediction')\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def create_model(image_shape, num_classes):\n",
    "    base_model = create_base_model(image_shape)\n",
    "    prediction_layer = create_prediction_layer(num_classes)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=image_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_fine_tune_model(model, fine_tune_at=100):\n",
    "    model.layers[1].trainable = True\n",
    "    print(\"Number of layers in the base model: \", len(model.layers[1].layers))\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in model.layers[1].layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_and_fit(model, train_ds, valid_ds, hyperparams, start_epoch=0):\n",
    "    if start_epoch != 0:\n",
    "        total_epochs =  hyperparams['initial_epochs'] + hyperparams['fine_tune_epochs']\n",
    "        lr = hyperparams['learning_rate'] / 10\n",
    "    else:\n",
    "        total_epochs =  hyperparams['initial_epochs']\n",
    "        lr = hyperparams['learning_rate']\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=hyperparams['label_smoothing']),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=total_epochs,\n",
    "        initial_epoch=start_epoch,\n",
    "        validation_data=valid_ds,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def plot(acc, val_acc, loss, val_loss, initial_epochs=0):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1.0])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    if initial_epochs != 0:\n",
    "        plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "            plt.ylim(), label='Start Fine Tuning')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0, 1.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    if initial_epochs != 0:\n",
    "        plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "            plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(save_path, image_ids, predicted_labels):\n",
    "    results = image_ids.drop('image', axis=1)\n",
    "    results.columns = ['ID', 'Label']\n",
    "    results['Label'] = predicted_labels \n",
    "    results = results.sort_values('ID').reset_index(drop=True)\n",
    "    results.to_csv(RESULT_SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from datasets.mri_dataset import MriDataset\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "\n",
    "data_folder = 'data'\n",
    "dataset = 'mri_dataset'\n",
    "train_label = 'train_label.csv'\n",
    "train_folder = 'train'\n",
    "test_folder = 'test'\n",
    "\n",
    "RESULT_SAVE_PATH = 'submission.csv'\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "IMG_SIZE = (512, 512)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "SEED = 0\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "train_folds = tfds.load(\n",
    "    name='mri_dataset', \n",
    "    split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 20)], # 'train[:90%]' \n",
    "    download=False, \n",
    "    shuffle_files=False, \n",
    "    as_supervised=True\n",
    ")\n",
    "valid_folds = tfds.load(\n",
    "    name='mri_dataset', \n",
    "    split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 20)], # 'train[:-10%]' \n",
    "    download=False, \n",
    "    shuffle_files=False, \n",
    "    as_supervised=True\n",
    ")\n",
    "test_ds_raw, test_info_raw = tfds.load(\n",
    "    name='mri_dataset', \n",
    "    split='test', \n",
    "    download=False, \n",
    "    shuffle_files=False, \n",
    "    as_supervised=False, \n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_folds = [ preprocess(ds, batch_size=TRAIN_BATCH_SIZE, ds_type='train') for ds in train_folds ]\n",
    "valid_folds = [ preprocess(ds, batch_size=VALID_BATCH_SIZE, ds_type='valid') for ds in valid_folds ]\n",
    "test_ds = preprocess(test_ds_raw, batch_size=TEST_BATCH_SIZE, ds_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of train batches: 17\nNumber of valid batches: 2\nNumber of test batches: 5\n"
     ]
    }
   ],
   "source": [
    "train_valid_df = pd.read_csv(os.path.join(data_folder, dataset, train_label))\n",
    "train_ds = train_folds[0]\n",
    "valid_ds = valid_folds[0]\n",
    "\n",
    "print(f'Number of train batches: {train_ds.cardinality()}')\n",
    "print(f'Number of valid batches: {valid_ds.cardinality()}')\n",
    "print(f'Number of test batches: {test_ds.cardinality()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:23:24.158035Z",
     "iopub.status.busy": "2020-09-17T01:23:24.157355Z",
     "iopub.status.idle": "2020-09-17T01:23:25.143573Z",
     "shell.execute_reply": "2020-09-17T01:23:25.144114Z"
    },
    "id": "K5BeQyKThC_Y"
   },
   "outputs": [],
   "source": [
    "# Train Data\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(9):\n",
    "#     for i in range(9):\n",
    "#       ax = plt.subplot(3, 3, i + 1)\n",
    "#       plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#       plt.title(labels[i].numpy())\n",
    "#       plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i, ds in enumerate(test_ds_raw.take(9)):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(ds['image'].numpy().astype(\"uint8\"))\n",
    "#     plt.title('ID: {}'.format(ds['id'].numpy()))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:23:25.195358Z",
     "iopub.status.busy": "2020-09-17T01:23:25.194720Z",
     "iopub.status.idle": "2020-09-17T01:23:26.186693Z",
     "shell.execute_reply": "2020-09-17T01:23:26.187234Z"
    },
    "id": "aQullOUHkm67"
   },
   "outputs": [],
   "source": [
    "# data_augmentation = create_augmentation_layer()\n",
    "\n",
    "# for image, _ in train_ds.take(1):\n",
    "#   plt.figure(figsize=(10, 10))\n",
    "#   first_image = image[0]\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "#     plt.imshow(augmented_image[0] / 255)\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Train and Validate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# -------------------- 1 fold -------------------- #\n",
      "1 fold pre train:\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 512, 512, 3)]     0         \n",
      "_________________________________________________________________\n",
      "xception (Functional)        (None, 16, 16, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "prediction (Sequential)      (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 20,867,627\n",
      "Trainable params: 6,147\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'callbacks' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8e37da1b3d3c>\u001b[0m in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, train_ds, valid_ds, hyperparams, start_epoch)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'callbacks' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "hyperparams = {\n",
    "    'initial_epochs': 150,\n",
    "    'fine_tune_epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'label_smoothing': 0.1,\n",
    "    'fine_tune_at': 10,\n",
    "}\n",
    "# fine_tune_lr: lr / 10\n",
    "# pred layer dropout: 0.3\n",
    "# lr decay steps/ decay rate\n",
    "\n",
    "train_accs, valid_accs, train_losses, valid_losses = [], [], [], []\n",
    "for i, (train_ds, valid_ds) in enumerate(zip(train_folds, valid_folds)):\n",
    "    k = i + 1\n",
    "    print(f'# -------------------- {k} fold -------------------- #')\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_model(IMG_SHAPE, num_classes)\n",
    "    \n",
    "    print(f'{k} fold pre train:') \n",
    "    model, history = compile_and_fit(model, train_ds, valid_ds, hyperparams)\n",
    "    print()\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    plot(acc, val_acc, loss, val_loss)\n",
    "    print()\n",
    "\n",
    "    print(f'{k} fold fine train:')\n",
    "    model = create_fine_tune_model(model, fine_tune_at=hyperparams['fine_tune_at'])\n",
    "    model, history_fine = compile_and_fit(model, train_ds, valid_ds, hyperparams, start_epoch=history.epoch[-1]) # TODO: Re-prefetch ds\n",
    "    print()\n",
    "\n",
    "    acc += history_fine.history['accuracy']\n",
    "    val_acc += history_fine.history['val_accuracy']\n",
    "    loss += history_fine.history['loss']\n",
    "    val_loss += history_fine.history['val_loss']\n",
    "    plot(acc, val_acc, loss, val_loss, initial_epochs=hyperparams['initial_epochs'])\n",
    "    print()\n",
    "\n",
    "    # record accuracy\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    valid_acc = history.history['val_accuracy'][-1]\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    valid_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f'{k} fold | Train Accuracy: {train_acc} | Validation Accuracy: {valid_acc} | Train Loss: {train_loss} | Validation Loss: {valid_loss}\\n')\n",
    "\n",
    "# average accuracy\n",
    "avg_train_acc = np.mean(train_accs)\n",
    "avg_valid_acc = np.mean(valid_accs)\n",
    "avg_train_loss = np.mean(train_loss)\n",
    "avg_valid_loss = np.mean(valid_loss)\n",
    "\n",
    "print(f'Avg Train Accuracy: {avg_train_acc} | Avg Validation Accuracy: {avg_valid_acc} | Avg Train Loss: {avg_train_loss} | Avg Validation Loss: {avg_valid_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6cWgjgfrsn5"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# TODO: combine ds and retrain\n",
    "# combined_ds = train_folds[0].concatenate(valid_folds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:31:39.642976Z",
     "iopub.status.busy": "2020-09-17T01:31:39.642300Z",
     "iopub.status.idle": "2020-09-17T01:31:41.388899Z",
     "shell.execute_reply": "2020-09-17T01:31:41.389462Z"
    },
    "id": "RUNoQNgtfNgt"
   },
   "outputs": [],
   "source": [
    "# image_batch, label_batch = valid_ds.as_numpy_iterator().next()\n",
    "# predictions = model.predict_on_batch(image_batch)\n",
    "# predicted_indices = tf.argmax(predictions, 1)\n",
    "# predicted_labels = predicted_indices.numpy()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#   ax = plt.subplot(3, 3, i + 1)\n",
    "#   plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "#   plt.title(f'pred: {predicted_labels[i]} true: {label_batch[i]}')\n",
    "#   plt.axis(\"off\")"
   ]
  },
  {
   "source": [
    "# Evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 2, 0, 2, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 2, 1,\n",
       "       1, 2, 1, 0, 2, 2, 1, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       2, 1, 2, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 2, 0, 0, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 2, 1, 1, 0, 2, 1,\n",
       "       1, 0, 1, 2, 1, 2, 1, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 1,\n",
       "       1, 0, 2, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 1, 0,\n",
       "       0, 2, 0, 0, 1, 2, 0, 0, 0, 2, 1, 1, 0, 1, 2, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 2, 0, 0, 0, 2, 0, 1, 2, 1, 1, 1, 0, 2, 2, 1, 2, 0, 2,\n",
       "       1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 2, 0, 1, 0, 2, 1, 0, 0,\n",
       "       2, 1, 1, 0, 1, 0, 2, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2, 0, 1, 2,\n",
       "       0, 0, 2, 0, 1, 2, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       2, 1, 0, 0, 0, 0, 1, 2, 1, 0, 2, 2, 0, 0, 0, 2, 2, 1, 2, 0, 0, 2,\n",
       "       0, 2, 0, 2, 1, 1, 1, 1, 1, 2, 0, 1, 0, 0, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "       0, 2, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "predictions = model.predict(test_ds)\n",
    "predicted_indices = tf.argmax(predictions, 1)\n",
    "predicted_labels = predicted_indices.numpy()\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = tfds.as_dataframe(test_ds_raw, test_info_raw)\n",
    "save_results(RESULT_SAVE_PATH, img_ids, predicted_labels)"
   ]
  },
  {
   "source": [
    "# Main"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# train\n",
    "    # kfold\n",
    "    # whole ds\n",
    "# evaluate"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}