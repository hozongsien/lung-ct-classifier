{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:23:10.839939Z",
     "iopub.status.busy": "2020-09-17T01:23:10.839289Z",
     "iopub.status.idle": "2020-09-17T01:23:18.333141Z",
     "shell.execute_reply": "2020-09-17T01:23:18.332497Z"
    },
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow_addons as tfa\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v77rlkCKW0IJ"
   },
   "source": [
    "# data_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds, batch_size, ds_type='train'):\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    num_classes = 3\n",
    "\n",
    "    resize_and_rescale = create_resize_and_rescale_layer()\n",
    "    data_augmentation = create_augmentation_layer()\n",
    "\n",
    "    if ds_type == 'test':\n",
    "        ds = ds.map(lambda ds: (ds['image'], ds['label']))\n",
    "\n",
    "    ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE) # disable to visualise train/ valid iamges\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if ds_type == 'train':\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if ds_type == 'train' or ds_type == 'valid': \n",
    "        ds = ds.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)))\n",
    "    \n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_resize_and_rescale_layer(): \n",
    "    resize_and_rescale = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE[0], IMG_SIZE[1], interpolation='bilinear'),\n",
    "        tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
    "    ], name='resize_and_rescale')\n",
    "    return resize_and_rescale\n",
    "\n",
    "\n",
    "def create_augmentation_layer():\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "        # tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.1),\n",
    "        # RandomInvert(),\n",
    "        # RandomSheer(),\n",
    "        # RandomGaussionFilter(),\n",
    "    ], name='data_augmentation')\n",
    "    return data_augmentation\n",
    "\n",
    "\n",
    "def make_generator(seed=None):\n",
    "    if seed:\n",
    "        return tf.random.Generator.from_seed(seed)\n",
    "    else:\n",
    "        return tf.random.Generator.from_non_deterministic_state()\n",
    "\n",
    "\n",
    "def random_invert_img(x, p=0.5):\n",
    "    if tf.random.uniform([]) < p:\n",
    "        x = (255-x)\n",
    "    else:\n",
    "        x\n",
    "    return x\n",
    "\n",
    "\n",
    "class RandomInvert(tf.keras.layers.Layer):\n",
    "    def __init__(self, factor=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor = factor\n",
    "    \n",
    "    def call(self, x):\n",
    "        return random_invert_img(x)\n",
    "\n",
    "\n",
    "\n",
    "class RandomSheer(tf.keras.layers.Layer):\n",
    "    def __init__(self, seed=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.rng = make_generator(seed)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x_level = self.rng.uniform(shape=[], minval=0.1, maxval=0.35)\n",
    "        y_level = self.rng.uniform(shape=[], minval=0.1, maxval=0.35)\n",
    "\n",
    "        x = tfa.image.shear_x(x, x_level, 0)\n",
    "        x = tfa.image.shear_y(x, y_level, 0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomGaussionFilter(tf.keras.layers.Layer):\n",
    "    def __init__(self, seed=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        k = np.random.randint(3, 11)\n",
    "        sigma = np.random.uniform(low=0.1, high=0.9)\n",
    "        x = tfa.image.gaussian_filter2d(x, filter_shape=(k, k), sigma=sigma)\n",
    "        return x\n",
    "\n",
    "\n",
    "def plot(acc, val_acc, loss, val_loss, initial_epochs=0):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1.0])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    if initial_epochs != 0:\n",
    "        plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "            plt.ylim(), label='Start Fine Tuning')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0, 1.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    if initial_epochs != 0:\n",
    "        plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "            plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(save_path, image_ids, predicted_labels):\n",
    "    results = image_ids.drop('image', axis=1)\n",
    "    results.columns = ['ID', 'Label']\n",
    "    results['Label'] = predicted_labels \n",
    "    results = results.sort_values('ID').reset_index(drop=True)\n",
    "    results.to_csv(RESULT_SAVE_PATH, index=False)"
   ]
  },
  {
   "source": [
    "# model.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(img_shape):\n",
    "    # base_model = tf.keras.applications.MobileNetV2(\n",
    "    #     input_shape=img_shape,\n",
    "    #     include_top=False,\n",
    "    #     weights='imagenet'\n",
    "    # )\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        input_shape=img_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "    )\n",
    "    # base_model = tf.keras.applications.ResNet152V2(\n",
    "    #     input_shape=img_shape,\n",
    "    #     include_top=False,\n",
    "    #     weights='imagenet',\n",
    "    # )\n",
    "    base_model.trainable = False\n",
    "    return base_model\n",
    "\n",
    "\n",
    "class AdaptiveConcatPooling(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AdaptiveConcatPooling, self).__init__()\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        output_size = (x.shape[1], x.shape[2])\n",
    "        avg_pool = tfa.layers.AdaptiveAveragePooling2D(output_size)(x)\n",
    "        max_pool = tfa.layers.AdaptiveMaxPooling2D(output_size)(x)\n",
    "        return tf.concat([avg_pool, max_pool], axis=1)\n",
    "\n",
    "\n",
    "def create_prediction_layer(num_classes):\n",
    "    # Float 32\n",
    "    # prediction = tf.keras.Sequential([\n",
    "    #     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    #     tf.keras.layers.Dropout(0.5),\n",
    "    #     tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
    "    # ], name='prediction')\n",
    "\n",
    "    # Mixed precision \n",
    "    prediction = tf.keras.Sequential([\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, name='logits'),\n",
    "        tf.keras.layers.Activation('softmax', dtype='float32', name='probs'),\n",
    "    ], name='prediction')\n",
    "    \n",
    "    # Adaptive\n",
    "    # prediction = tf.keras.Sequential([\n",
    "    #     AdaptiveConcatPooling(),\n",
    "    #     tf.keras.layers.Flatten(),\n",
    "    #     tf.keras.layers.BatchNormalization(),\n",
    "    #     tf.keras.layers.Dropout(0.75),\n",
    "    #     tf.keras.layers.Dense(num_classes, name='logits'),\n",
    "    #     tf.keras.layers.Activation('softmax', dtype='float32', name='probs'),\n",
    "    # ], name='prediction')\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def create_model(image_shape, num_classes):\n",
    "    base_model = create_base_model(image_shape)\n",
    "    prediction_layer = create_prediction_layer(num_classes)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=image_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_fine_tune_model(model, fine_tune_at=100):\n",
    "    print(\"Number of layers in the base model: \", len(model.layers[1].layers))\n",
    "    model.layers[1].trainable = True\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in model.layers[1].layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Unfreeze the top layers while leaving BatchNorm layers frozen\n",
    "    # for layer in model.layers[1].layers[-fine_tune_at:]: \n",
    "    #     if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "    #         layer.trainable = True\n",
    "    #         print(layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "# train.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, train_ds, valid_ds, hyperparams, start_epoch=0, name=''):\n",
    "    if start_epoch != 0:\n",
    "        total_epochs =  hyperparams['initial_epochs'] + hyperparams['fine_epochs']\n",
    "        lr = hyperparams['fine_learning_rate']\n",
    "    else:\n",
    "        total_epochs =  hyperparams['initial_epochs']\n",
    "        lr = hyperparams['learning_rate']\n",
    "\n",
    "    log_dir = os.path.join('logs', name)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=hyperparams['label_smoothing']),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=total_epochs,\n",
    "        initial_epoch=start_epoch,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[tensorboard_callback],\n",
    "        # verbose=0,\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "source": [
    "# main.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre pre-process images\n",
    "# python preprocess.py\n",
    "\n",
    "# Build dataset\n",
    "# python -m tensorflow_datasets.scripts.download_and_prepare --datasets=mri_dataset --module_import=datasets.mri_dataset --manual_dir=data/processed --data_dir=data/"
   ]
  },
  {
   "source": [
    "## GPU Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable first GPU\n",
    "  tf.config.set_visible_devices(physical_devices[1:], 'GPU')\n",
    "  logical_devices = tf.config.list_logical_devices('GPU')\n",
    "  # Logical device was not created for first GPU\n",
    "  assert len(logical_devices) == len(physical_devices) - 1\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "source": [
    "## Enable Mixed Precision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\nSome of your GPUs may run slowly with dtype policy mixed_float16 because they do not all have compute capability of at least 7.0. Your GPUs:\n  Tesla V100S-PCIE-32GB, compute capability 7.0\n  Tesla V100-PCIE-32GB, compute capability 7.0\n  Tesla P100-PCIE-16GB, compute capability 6.0 (x2)\nSee https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\nIf you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "source": [
    "## Load Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from datasets.mri_dataset import MriDataset\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "data_folder = 'data'\n",
    "raw_folder = 'raw'\n",
    "processed_folder = 'processed'\n",
    "dataset = 'mri_dataset'\n",
    "train_folder = 'train'\n",
    "test_folder = 'test'\n",
    "train_label = 'train_label.csv'\n",
    "\n",
    "RESULT_SAVE_PATH = 'submission.csv'\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 64\n",
    "IMG_SIZE = (512, 512)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "SEED = 0\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "num_classes = 3\n",
    "\n",
    "train_folds = tfds.load(\n",
    "    name='mri_dataset', \n",
    "    split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 20)],\n",
    "    download=False, \n",
    "    shuffle_files=False, \n",
    "    as_supervised=True,\n",
    "    data_dir=data_folder\n",
    ")\n",
    "valid_folds = tfds.load(\n",
    "    name='mri_dataset', \n",
    "    split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 20)],\n",
    "    download=False, \n",
    "    shuffle_files=False, \n",
    "    as_supervised=True,\n",
    "    data_dir=data_folder\n",
    ")\n",
    "test_ds_raw, test_info_raw = tfds.load(\n",
    "    name='mri_dataset', \n",
    "    split='test', \n",
    "    download=False, \n",
    "    shuffle_files=False, \n",
    "    as_supervised=False, \n",
    "    with_info=True,\n",
    "    data_dir=data_folder\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_folds = [ preprocess(ds, batch_size=TRAIN_BATCH_SIZE, ds_type='train') for ds in train_folds ]\n",
    "valid_folds = [ preprocess(ds, batch_size=VALID_BATCH_SIZE, ds_type='valid') for ds in valid_folds ]\n",
    "test_ds = preprocess(test_ds_raw, batch_size=TEST_BATCH_SIZE, ds_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of train batches: 17\nNumber of valid batches: 2\nNumber of test batches: 5\n"
     ]
    }
   ],
   "source": [
    "train_valid_df = pd.read_csv(os.path.join(data_folder, processed_folder, train_label))\n",
    "train_ds = train_folds[0]\n",
    "valid_ds = valid_folds[0]\n",
    "\n",
    "print(f'Number of train batches: {train_ds.cardinality()}')\n",
    "print(f'Number of valid batches: {valid_ds.cardinality()}')\n",
    "print(f'Number of test batches: {test_ds.cardinality()}')"
   ]
  },
  {
   "source": [
    "## Visualise Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:23:24.158035Z",
     "iopub.status.busy": "2020-09-17T01:23:24.157355Z",
     "iopub.status.idle": "2020-09-17T01:23:25.143573Z",
     "shell.execute_reply": "2020-09-17T01:23:25.144114Z"
    },
    "id": "K5BeQyKThC_Y"
   },
   "outputs": [],
   "source": [
    "# # disable image resize and rescale in preprocess function ONLY for visualisation\n",
    "# # Train Data\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(9):\n",
    "#     for i in range(9):\n",
    "#       ax = plt.subplot(3, 3, i + 1)\n",
    "#       plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#       plt.title(labels[i].numpy())\n",
    "#       plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Data\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i, ds in enumerate(test_ds_raw.take(9)):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(ds['image'].numpy().astype(\"uint8\"))\n",
    "#     plt.title('ID: {}'.format(ds['id'].numpy()))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:23:25.195358Z",
     "iopub.status.busy": "2020-09-17T01:23:25.194720Z",
     "iopub.status.idle": "2020-09-17T01:23:26.186693Z",
     "shell.execute_reply": "2020-09-17T01:23:26.187234Z"
    },
    "id": "aQullOUHkm67"
   },
   "outputs": [],
   "source": [
    "# TODO: Fix visualisation\n",
    "# data_augmentation = create_augmentation_layer()\n",
    "\n",
    "# for image, _ in train_ds.take(1):\n",
    "#   plt.figure(figsize=(10, 10))\n",
    "#   first_image = image[0]\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "#     plt.imshow(augmented_image[0] / 255)\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "source": [
    "## Train and Validate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# -------------------- 1 fold -------------------- #\n",
      "1 fold pre train:\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 512, 512, 3)]     0         \n",
      "_________________________________________________________________\n",
      "xception (Functional)        (None, 16, 16, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "prediction (Sequential)      (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 20,867,627\n",
      "Trainable params: 6,147\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/zongsien/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:From /home/zongsien/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1de77bf6f2b2>\u001b[0m in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, train_ds, valid_ds, hyperparams, start_epoch, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/MedicalImageClassifier/env/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "hyperparams = {\n",
    "    'initial_epochs': 150,\n",
    "    'fine_epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'fine_learning_rate': 1e-5,\n",
    "    'label_smoothing': 0.1,\n",
    "    'fine_tune_at': 10,\n",
    "}\n",
    "\n",
    "train_accs, valid_accs, train_losses, valid_losses = [], [], [], []\n",
    "train_folds = [train_folds[3]] + train_folds[0:3] + [train_folds[4]] # re-order folds because fold-4 hardest to train\n",
    "valid_folds = [valid_folds[3]] + valid_folds[0:3] + [valid_folds[4]]\n",
    "for i, (train_ds, valid_ds) in enumerate(zip(train_folds, valid_folds)):\n",
    "    k = i + 1\n",
    "    experiment_name = f'inception-{k}_fold'\n",
    "    print(f'# -------------------- {k} fold -------------------- #')\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_model(IMG_SHAPE, num_classes)\n",
    "    \n",
    "    print(f'{k} fold pre train:') \n",
    "    model, history = compile_and_fit(model, train_ds, valid_ds, hyperparams, name=experiment_name)\n",
    "    print()\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    plot(acc, val_acc, loss, val_loss)\n",
    "    print()\n",
    "\n",
    "    print(f'{k} fold fine train:')\n",
    "    model = create_fine_tune_model(model, fine_tune_at=hyperparams['fine_tune_at'])\n",
    "    model, history_fine = compile_and_fit(model, train_ds, valid_ds, hyperparams, start_epoch=history.epoch[-1], name=experiment_name)\n",
    "    print()\n",
    "\n",
    "    acc += history_fine.history['accuracy']\n",
    "    val_acc += history_fine.history['val_accuracy']\n",
    "    loss += history_fine.history['loss']\n",
    "    val_loss += history_fine.history['val_loss']\n",
    "    plot(acc, val_acc, loss, val_loss, initial_epochs=hyperparams['initial_epochs']+1)\n",
    "    print()\n",
    "\n",
    "    # record accuracy\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    valid_acc = history.history['val_accuracy'][-1]\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    valid_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f'{k} fold | Train Loss: {train_loss} | Train Accuracy: {train_acc} | Validation Loss: {valid_loss} | Validation Accuracy: {valid_acc}\\n')\n",
    "\n",
    "# average accuracy\n",
    "avg_train_acc = np.mean(train_accs)\n",
    "avg_valid_acc = np.mean(valid_accs)\n",
    "avg_train_loss = np.mean(train_loss)\n",
    "avg_valid_loss = np.mean(valid_loss)\n",
    "\n",
    "print(f'Avg Train Loss: {avg_train_loss} | Avg Train Accuracy: {avg_train_acc} | Avg Validation Loss: {avg_valid_loss} | Avg Validation Accuracy: {avg_valid_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6cWgjgfrsn5"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# TODO: combine ds and retrain with all data\n",
    "# combined_ds = train_folds[0].concatenate(valid_folds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T01:31:39.642976Z",
     "iopub.status.busy": "2020-09-17T01:31:39.642300Z",
     "iopub.status.idle": "2020-09-17T01:31:41.388899Z",
     "shell.execute_reply": "2020-09-17T01:31:41.389462Z"
    },
    "id": "RUNoQNgtfNgt"
   },
   "outputs": [],
   "source": [
    "# TODO: analyse predicted results\n",
    "# image_batch, label_batch = valid_ds.as_numpy_iterator().next()\n",
    "# predictions = model.predict_on_batch(image_batch)\n",
    "# predicted_indices = tf.argmax(predictions, 1)\n",
    "# predicted_labels = predicted_indices.numpy()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#   ax = plt.subplot(3, 3, i + 1)\n",
    "#   plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "#   plt.title(f'pred: {predicted_labels[i]} true: {label_batch[i]}')\n",
    "#   plt.axis(\"off\")"
   ]
  },
  {
   "source": [
    "## Evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 2, 0, 2, 1, 0, 2, 2, 1, 1, 0, 1, 2, 0, 0, 1, 1, 2, 1,\n",
       "       1, 2, 1, 0, 2, 2, 0, 0, 2, 2, 2, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       2, 1, 2, 0, 2, 0, 2, 1, 0, 1, 0, 1, 0, 1, 1, 1, 2, 2, 0, 0, 1, 1,\n",
       "       2, 0, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 0, 1, 2, 1, 1, 0, 2, 1,\n",
       "       1, 0, 1, 2, 1, 2, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1,\n",
       "       1, 0, 2, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 1, 0, 1, 1,\n",
       "       1, 2, 1, 0, 1, 2, 0, 0, 0, 2, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 0, 2,\n",
       "       1, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 0, 2, 2, 0, 1, 0, 2, 0, 0, 0,\n",
       "       2, 1, 1, 0, 1, 0, 2, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2, 0, 1, 2,\n",
       "       0, 1, 2, 0, 1, 2, 1, 0, 1, 1, 1, 2, 2, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 1, 1, 0, 0, 2,\n",
       "       0, 2, 0, 2, 1, 1, 1, 0, 1, 2, 0, 1, 0, 0, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "       0, 2, 0, 2, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# predict test labels\n",
    "predictions = model.predict(test_ds)\n",
    "predicted_indices = tf.argmax(predictions, 1)\n",
    "predicted_labels = predicted_indices.numpy()\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "img_ids = tfds.as_dataframe(test_ds_raw, test_info_raw)\n",
    "save_results(RESULT_SAVE_PATH, img_ids, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}